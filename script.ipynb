{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import sqlite3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"product\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"offers\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing location by automating browser\n",
    "driver = webdriver.Chrome(\"D:\\Selenium\\chromedriver.exe\")\n",
    "driver.get('https://www.pizzahutbd.com/deals/all')\n",
    "driver.find_element(By.CLASS_NAME, 'tab-content').click()\n",
    "\n",
    "loc_input = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, 'pac-input')))\n",
    "loc_input.click()\n",
    "loc_input.send_keys('Dhaka')\n",
    "time.sleep(5)\n",
    "loc_input.send_keys(Keys.ARROW_DOWN)\n",
    "loc_input.send_keys(Keys.ENTER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep 10 sec to load the page proparly \n",
    "time.sleep(10)\n",
    "soup = BeautifulSoup(driver.page_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract data for different products from PizzaHut\n",
    "def ETL(soup2):\n",
    "    category = soup2.find('a', attrs={'class':'nav-link active'}).text\n",
    "    page_name = 'PizzaHut'\n",
    "    \n",
    "    items = soup2.find_all('div', class_ = 'pizzaPrice')\n",
    "    prices = []\n",
    "    \n",
    "    csv_file = open(f'product/{category}.csv','w', newline = '')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['name', 'description', 'category', 'price','page_name', 'loc'])\n",
    "    \n",
    "    items = soup2.find_all('div', class_ = 'container thame collapseExample')\n",
    "    \n",
    "    count = 0\n",
    "    for item in items:\n",
    "        name = item.find('div', class_ = 'left-con-pizzas').text\n",
    "        type = item.find('label', class_ = 'product_size_label').text\n",
    "        name = name+' - '+type\n",
    "        description = item.find('p', class_ = 'short_desc').text\n",
    "        price = item.find('span', class_ = 'pro_price').text\n",
    "        category = category\n",
    "        page_name = page_name\n",
    "        loc = 'Dhaka' # setting default loc to Dhaka\n",
    "        count += 1\n",
    "        csv_writer.writerow([name, description, category, price,page_name, loc])\n",
    "    \n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_ = ['pasta', 'appetisers', 'drinks']\n",
    "for i in list_:\n",
    "   driver.get(f'https://www.pizzahutbd.com/{i}/all') \n",
    "   time.sleep(10)\n",
    "   soup2 = BeautifulSoup(driver.page_source)\n",
    "   ETL(soup2)\n",
    "# Storing offer page for pizzaHut\n",
    "driver.get('https://www.pizzahutbd.com/deals/all') \n",
    "hutOffer = BeautifulSoup(driver.page_source)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_name = 'PizzaHut'\n",
    "category = soup.find('a', attrs={'class':'nav-link active'}).text\n",
    "\n",
    "items = soup.find_all('div', class_ = 'pizzaPrice')\n",
    "prices = []\n",
    "\n",
    "for i in items:\n",
    "    prices.append(i.text)\n",
    "    \n",
    "price_list =  [' '.join(prices[i:i+3]) for i in range(0, len(prices), 3)]\n",
    "\n",
    "csv_file = open('product/Pizza.csv','w', newline = '')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['name', 'description', 'category', 'price','page_name', 'loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = soup.find_all('div', class_ = 'container thame collapseExample')\n",
    "\n",
    "count = 0\n",
    "for item in items:\n",
    "    name = item.find('div', class_ = 'left-con-pizzas').text\n",
    "    description = item.find('p', class_ = 'short_desc').text\n",
    "    price = price_list[count]\n",
    "    category = category\n",
    "    page_name = page_name\n",
    "    loc = 'Dhaka'\n",
    "    count += 1\n",
    "    csv_writer.writerow([name, description, category, price,page_name, loc])\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('product/Pizza.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Personal_S_Price', 'price', 'Large_S_Price']] = df['price'].str.split(' ', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('product/Pizza.csv')\n",
    "df = pd.read_csv('product/Pizza.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://madchef.com.bd/menu\"\n",
    "page = requests.get(URL).text\n",
    "\n",
    "soup = BeautifulSoup(page, features=\"xml\")\n",
    "\n",
    "page_name = 'Madchef'\n",
    "items = soup.find_all('div', class_ = 'menumodal modal')\n",
    "\n",
    "csv_file = open('product/Madchef.csv','w', newline = '')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['name', 'description', 'category', 'price','page_name', 'loc'])\n",
    "\n",
    "# finding locs of Madchef's\n",
    "\n",
    "loc_page = requests.get('https://madchef.com.bd/contact#branches').text\n",
    "soup_loc = BeautifulSoup(loc_page, features=\"xml\")\n",
    "\n",
    "locs = soup_loc.find_all('div', class_ ='branch-name ')\n",
    "locations=[]\n",
    "for i in locs:\n",
    "    locations.append((i.text).strip())\n",
    "\n",
    "for item in items:\n",
    "\n",
    "    name = (item.find('div', class_='menumodal-title').text).strip()\n",
    "    if name.split()[-1] == 'ðŸŒ¶':\n",
    "        name = ' '.join(name.split()[:-1])\n",
    "        \n",
    "    description = (item.find('div', class_='menumodal-description').text).strip()\n",
    "    category = ((item.find('div', class_='menumodal-category').text).strip().split(':')[1]).strip()\n",
    "    price = (item.find('div', class_='menumodal-price').text).strip().split()[1]\n",
    "    page_name = page_name\n",
    "    loc = locations\n",
    "\n",
    "    csv_writer.writerow([name, description, category, price, page_name, loc])\n",
    "\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "df = pd.read_csv('product/Madchef.csv')\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offers Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_name = 'PizzaHut'\n",
    "\n",
    "csv_file = open(f'offers/hut.csv','w', newline = '')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['name', 'description', 'category', 'price','page_name'])\n",
    "\n",
    "items = hutOffer.find_all('div', class_ = 'deals_card')\n",
    "for item in items:\n",
    "    name = (item.find('div', class_ = 'deal-item-name').text).strip()\n",
    "    description = (item.find('div', class_ = 'deal-item-desc').text).strip()\n",
    "    category = 'Pizza'\n",
    "    page_name = page_name\n",
    "    csv_writer.writerow([name, description, category, page_name])\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filepath= './product/'\n",
    "\n",
    "filenames = list(filter(lambda x: '.csv' in x, os.listdir(filepath)))\n",
    "li = []\n",
    "\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(filepath+filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.tail(50)\n",
    "df = frame.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1, errors='ignore')\n",
    "df['Personal_S_Price'] = df['Personal_S_Price'].fillna(0)\n",
    "df['Large_S_Price'] = df['Large_S_Price'].fillna(0)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db.sqlite3')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('CREATE TABLE IF NOT EXISTS products (name text, description text, price number, page_name text, low_price number, high_price number)')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('CREATE TABLE IF NOT EXISTS offers (name text, description text, category text, page_name text)')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer = pd.read_csv('offers/hut.csv')\n",
    "offer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer.to_sql('offers', conn, if_exists='replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('products', conn, if_exists='replace', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DataEng': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b13706885e49f93c305cd8554f3e96068d9c30cbac4d019593d291d75669e384"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
