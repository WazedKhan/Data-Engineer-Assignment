Concerns: was different website use different information so for all of them creating a table structure so that all data can be stored and
          can be queried.

Codes: All the information is given in the notebook version of the code.

Approaches: After visiting sites approach was to start will a simple one after that start with a difficult one so
            that some data is on my database and I can keep working on the rest even if it takes time.
            The first one was simple Madchife doesn't require any input or session to collect data from it so after collecting data
            from went to PizzaHut
            As PizzaHut requires session and input so I used selenium for it. As for Chillox, it requires OCR but the structure of
            the Optical object is not arranged so it needs to get data in a format.

Outputs: it was as expected except for some which later need to modifications.

Test Cases:
    (1) Number of item were on site and number of item bot was scrapping
    (2) Type of the item bot was scraping
    (3) Scraped item was NaN or not
    (4) If Item contains any special Chars

Challenges: all the data that was collected was unstructured and data was comming from different source so it was a Challenge to create a
            structure out of them.


API:
    |_ __pycache__
    |_ db.sqlite3
    |_ main.py

    __pycache__ :It is a directory that contains bytecode cache files that are automatically generated by python.

    db.sqlite3 :It contains data that were scraped from sites.

    main.py : is FAST_API File for running a server to provide API

    "get_all_proucts" is function that gets all the proucts that is stored in product table in db.sqlite3 database and return json format for that data.

    "get_all_offers" is function that gets all the proucts that is stored in offers table in db.sqlite3 database and return json format for that data.

    ----------------------------------------------------
    @app.get("/products")
    async def root():
        return {"message": get_all_proucts()}

    is a fast API fuction that gets call at route "http://127.0.0.1:8000/products" that collects json data from get_all_products and returnes It.

    _____________________________________________________
    @app.get("/offers")
    async def root():
        return {"message": get_all_offers()}

    is a fast API fuction that gets call at route "http://127.0.0.1:8000/offers" that collects json data from get_all_offers and returnes It.

_____________________________________________________
| offers and product |- folder contains csv files which contains scraped data from sites

_____________________________________________________
db.sqlite3 file is the database files that store collected data from sites so that different SQL queries can be done on them.

_____________________________________________________
requirements.txt file contains all the Dependencies that are required to run the scripts

_____________________________________________________
script.ipynb is nootbook file for scraping data from sites and store them into the db.sqlite3 and script.py is the python file same function
you can use any of the file both dose the same thing.

_____________________________________________________
